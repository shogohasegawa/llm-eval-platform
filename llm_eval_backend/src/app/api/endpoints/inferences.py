"""
æ¨è«–ç®¡ç†APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

æ¨è«–ã®CRUDæ“ä½œã¨å®Ÿè¡Œã‚’æä¾›ã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚
"""
import json
import logging
import uuid
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from fastapi import APIRouter, HTTPException, status, Depends, BackgroundTasks, Query, Path

from app.api.models import (
    Inference, 
    InferenceCreate, 
    InferenceUpdate, 
    InferenceStatus, 
    InferenceResult,
    InferenceListResponse,
    InferenceDetailResponse,
    ModelConfig,
    EvaluationRequest
)
from app.utils.db.models import get_model_repository
from app.utils.db.providers import get_provider_repository
from app.utils.db.inferences import get_inference_repository
from app.core.evaluation import run_multiple_evaluations
from app.utils.litellm_helper import get_provider_options

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger("llmeval")

router = APIRouter(prefix="/inferences", tags=["inferences"])


@router.get("", response_model=List[Inference])
async def list_inferences(
    dataset_id: Optional[str] = None,
    provider_id: Optional[str] = None,
    model_id: Optional[str] = None,
    status: Optional[str] = None
):
    """
    æ¨è«–ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        dataset_id: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿
        provider_id: ãƒ—ãƒ­ãƒã‚¤ãƒ€IDã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿
        model_id: ãƒ¢ãƒ‡ãƒ«IDã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿
        status: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿
        
    Returns:
        List[Inference]: æ¨è«–ä¸€è¦§
    """
    try:
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¡ä»¶ã‚’ä½œæˆ
        filters = {}
        if dataset_id:
            filters["dataset_id"] = dataset_id
        if provider_id:
            filters["provider_id"] = provider_id
        if model_id:
            filters["model_id"] = model_id
        if status:
            filters["status"] = status
        
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ä¸€è¦§ã‚’å–å¾—
        inference_repo = get_inference_repository()
        inferences = inference_repo.get_all_inferences(filters)
        
        # APIå¿œç­”ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        result = []
        for inf in inferences:
            # çµæœã‚’å¤‰æ›
            results = []
            for res in inf.get("results", []):
                results.append(InferenceResult(
                    id=res["id"],
                    inference_id=res["inference_id"],
                    input=res["input"],
                    expected_output=res.get("expected_output"),
                    actual_output=res["actual_output"],
                    metrics=res.get("metrics"),
                    latency=res.get("latency"),
                    token_count=res.get("token_count"),
                    created_at=datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
                ))
            
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
            created_at = datetime.fromisoformat(inf["created_at"]) if isinstance(inf["created_at"], str) else inf["created_at"]
            updated_at = datetime.fromisoformat(inf["updated_at"]) if isinstance(inf["updated_at"], str) else inf["updated_at"]
            completed_at = None
            if inf.get("completed_at"):
                completed_at = datetime.fromisoformat(inf["completed_at"]) if isinstance(inf["completed_at"], str) else inf["completed_at"]
            
            # æ¨è«–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            inference = Inference(
                id=inf["id"],
                name=inf["name"],
                description=inf.get("description"),
                dataset_id=inf["dataset_id"],
                provider_id=inf["provider_id"],
                model_id=inf["model_id"],
                status=InferenceStatus(inf["status"]),
                progress=inf["progress"],
                metrics=inf.get("metrics"),
                results=results,
                created_at=created_at,
                updated_at=updated_at,
                completed_at=completed_at,
                error=inf.get("error")
            )
            result.append(inference)
        
        return result
    except Exception as e:
        logger.error(f"æ¨è«–ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


@router.post("", response_model=Inference, status_code=status.HTTP_201_CREATED)
async def create_inference(
    inference_data: InferenceCreate,
    background_tasks: BackgroundTasks
):
    """
    æ–°ã—ã„æ¨è«–ã‚’ä½œæˆã—ã€ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§è©•ä¾¡ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    
    Args:
        inference_data: ä½œæˆã™ã‚‹æ¨è«–ã®ãƒ‡ãƒ¼ã‚¿
        background_tasks: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
        
    Returns:
        Inference: ä½œæˆã•ã‚ŒãŸæ¨è«–æƒ…å ±
    """
    try:
        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not inference_data.dataset_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆIDã¯å¿…é ˆã§ã™"
            )
            
        if not inference_data.provider_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ãƒ—ãƒ­ãƒã‚¤ãƒ€IDã¯å¿…é ˆã§ã™"
            )
            
        if not inference_data.model_id:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="ãƒ¢ãƒ‡ãƒ«IDã¯å¿…é ˆã§ã™"
            )
            
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŠ½å‡ºï¼ˆãƒ‘ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—ï¼‰
        # "test/example.json" -> "example"
        dataset_name = inference_data.dataset_id.split('/')[-1].replace('.json', '')
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¨ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
        provider_repo = get_provider_repository()
        model_repo = get_model_repository()
        
        provider = provider_repo.get_provider_by_id(inference_data.provider_id)
        if not provider:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ID '{inference_data.provider_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        model = model_repo.get_model_by_id(inference_data.model_id)
        if not model:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ãƒ¢ãƒ‡ãƒ«ID '{inference_data.model_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        provider_type = provider["type"]
        model_name = model["name"]
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ§‹ç¯‰
        model_config = ModelConfig(
            provider=provider_type,  
            model_name=model_name,
            max_tokens=inference_data.max_tokens or 512,
            temperature=inference_data.temperature or 0.7,
            top_p=inference_data.top_p or 1.0
        )
        
        # è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ§‹ç¯‰
        evaluation_request = EvaluationRequest(
            datasets=[dataset_name],
            num_samples=inference_data.num_samples or 100,
            n_shots=[inference_data.n_shots or 0],
            model=model_config
        )
        
        logger.info(f"è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {evaluation_request}")
        
        # æ–°ã—ã„æ¨è«–ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        inference_repo = get_inference_repository()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
        parameters = {
            "max_tokens": inference_data.max_tokens or 512,
            "temperature": inference_data.temperature or 0.7,
            "top_p": inference_data.top_p or 1.0,
            "num_samples": inference_data.num_samples or 100,
            "n_shots": inference_data.n_shots or 0
        }
        
        # æ¨è«–ã‚’ä½œæˆï¼ˆåˆæœŸçŠ¶æ…‹ã¯PENDINGï¼‰
        inference_db = inference_repo.create_inference({
            "name": inference_data.name,
            "description": inference_data.description,
            "dataset_id": inference_data.dataset_id,
            "provider_id": inference_data.provider_id,
            "model_id": inference_data.model_id,
            "status": InferenceStatus.PENDING,
            "progress": 0,
            "parameters": parameters
        })
        
        # èƒŒæ™¯ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            execute_inference_evaluation,
            inference_id=inference_db["id"],
            evaluation_request=evaluation_request,
            provider_name=provider_type,
            model_name=model_name
        )
        
        # çµæœã‚’å¤‰æ›
        results = []
        for res in inference_db.get("results", []):
            results.append(InferenceResult(
                id=res["id"],
                inference_id=res["inference_id"],
                input=res["input"],
                expected_output=res.get("expected_output"),
                actual_output=res["actual_output"],
                metrics=res.get("metrics"),
                latency=res.get("latency"),
                token_count=res.get("token_count"),
                created_at=datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
            ))
        
        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
        created_at = datetime.fromisoformat(inference_db["created_at"]) if isinstance(inference_db["created_at"], str) else inference_db["created_at"]
        updated_at = datetime.fromisoformat(inference_db["updated_at"]) if isinstance(inference_db["updated_at"], str) else inference_db["updated_at"]
        completed_at = None
        if inference_db.get("completed_at"):
            completed_at = datetime.fromisoformat(inference_db["completed_at"]) if isinstance(inference_db["completed_at"], str) else inference_db["completed_at"]
        
        # æ¨è«–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
        inference = Inference(
            id=inference_db["id"],
            name=inference_db["name"],
            description=inference_db.get("description"),
            dataset_id=inference_db["dataset_id"],
            provider_id=inference_db["provider_id"],
            model_id=inference_db["model_id"],
            status=InferenceStatus(inference_db["status"]),
            progress=inference_db["progress"],
            metrics=inference_db.get("metrics"),
            results=results,
            created_at=created_at,
            updated_at=updated_at,
            completed_at=completed_at,
            error=inference_db.get("error")
        )
        
        return inference
        
    except Exception as e:
        logger.error(f"æ¨è«–ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


# èƒŒæ™¯ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè¡Œã™ã‚‹è©•ä¾¡é–¢æ•°
async def execute_inference_evaluation(
    inference_id: str,
    evaluation_request: EvaluationRequest,
    provider_name: str,
    model_name: str
):
    """
    æ¨è«–è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
    
    Args:
        inference_id: æ¨è«–ID
        evaluation_request: è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        provider_name: ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    logger.info(f"æ¨è«– {inference_id} ã®è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã™")
    inference_repo = get_inference_repository()
    
    try:
        # æ¨è«–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        inference_repo.update_inference(inference_id, {"status": InferenceStatus.RUNNING, "progress": 0})
        
        # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã”ã¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨ï¼‰
        additional_params = get_provider_options(provider_name)
        
        # è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³å‘¼ã³å‡ºã—
        results_full = await run_multiple_evaluations(
            datasets=evaluation_request.datasets,
            provider_name=provider_name,
            model_name=model_name,
            num_samples=evaluation_request.num_samples,
            n_shots=evaluation_request.n_shots,
            additional_params=additional_params
        )
        
        # ãƒ•ãƒ©ãƒƒãƒˆãªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸ã‚’ä½œæˆ
        flat_metrics: Dict[str, float] = {}
        for ds, ds_res in results_full.get("results", {}).items():
            details = ds_res.get("details", {})
            for key, value in details.items():
                if key.endswith("_details") or key.endswith("_error_rate"):
                    continue
                flat_metrics[key] = value
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒç©ºã®å ´åˆã¯è­¦å‘Š
        if not flat_metrics:
            logger.warning(f"æ¨è«– {inference_id} ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒç©ºã§ã™ã€‚è©•ä¾¡ãŒæ­£ã—ãè¡Œã‚ã‚Œãªã‹ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒç©ºã§ã‚‚å®Œäº†ã¨ã™ã‚‹ãŒã€ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨­å®š
            inference_repo.update_inference(inference_id, {
                "status": InferenceStatus.COMPLETED,
                "progress": 100,
                "metrics": {},
                "error": "ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ãŒé©åˆ‡ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            })
        else:
            # MLflowã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ã‚°
            try:
                # ãƒ­ã‚°ç”¨ã®å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«åã‚’æ§‹ç¯‰
                full_model_name = f"{provider_name}/{model_name}"
                logger.info(f"ğŸ”„ æ¨è«– {inference_id} ã®è©•ä¾¡çµæœã‚’MLflowã«è¨˜éŒ²ã—ã¾ã™: {full_model_name}")
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ­ã‚°
                metrics_sample = list(flat_metrics.items())[:5]
                logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¾‹ (5ä»¶): {metrics_sample}")
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”¨ï¼‰
                from app.utils.logging import log_evaluation_results
                import time
                metrics_log_file = f"/app/inference_metrics_{provider_name}_{model_name}_{int(time.time())}.json"
                with open(metrics_log_file, "w") as f:
                    json.dump({
                        "inference_id": inference_id,
                        "provider": provider_name,
                        "model": model_name,
                        "timestamp": datetime.now().isoformat(),
                        "metrics": flat_metrics
                    }, f, indent=2)
                logger.info(f"ğŸ“Š æ¨è«–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {metrics_log_file}")
                
                # MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°å®Ÿè¡Œï¼ˆãƒ‡ãƒãƒƒã‚°è©³ç´°ä»˜ãï¼‰
                from app.utils.logging import log_evaluation_results
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¿ã‚¤ãƒ—ã‚’ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                for key, value in flat_metrics.items():
                    logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å‹ç¢ºèª: {key}={value} (type: {type(value).__name__})")
                    
                    # å€¤ãŒæ•°å€¤ã§ãªã„å ´åˆã¯è£œæ­£
                    if not isinstance(value, (int, float)):
                        try:
                            flat_metrics[key] = float(value)
                            logger.info(f"ğŸ”„ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ•°å€¤å‹ã«å¤‰æ›: {key}={flat_metrics[key]}")
                        except (ValueError, TypeError):
                            logger.warning(f"âš ï¸ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ {key} ã‚’æ•°å€¤ã«å¤‰æ›ã§ãã¾ã›ã‚“: {value}")
                
                # MLflowã«ãƒ­ã‚°è¨˜éŒ²
                logging_result = await log_evaluation_results(
                    model_name=full_model_name,
                    metrics=flat_metrics
                )
                
                logger.info(f"âœ… MLflowã¸ã®ãƒ­ã‚°è¨˜éŒ²çµæœ: {logging_result}")
                
            except Exception as mlflow_error:
                error_message = str(mlflow_error)
                logger.error(f"âŒ MLflowã¸ã®ãƒ­ã‚°è¨˜éŒ²ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {error_message}", exc_info=True)
                
                # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
                import traceback
                error_log_file = f"/app/inference_mlflow_error_{provider_name}_{model_name}_{int(time.time())}.txt"
                with open(error_log_file, "w") as f:
                    f.write(f"Error logging metrics for inference {inference_id} ({provider_name}/{model_name}): {error_message}\n\n")
                    traceback.print_exc(file=f)
                logger.error(f"âŒ MLflowã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {error_log_file}")
            
            # æ¨è«–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å®Œäº†ã«æ›´æ–°ï¼ˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚ã‚Šï¼‰
            inference_repo.update_inference(inference_id, {
                "status": InferenceStatus.COMPLETED,
                "progress": 100,
                "metrics": flat_metrics,
                "error": None  # ã‚¨ãƒ©ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚¯ãƒªã‚¢
            })
        
        logger.info(f"æ¨è«– {inference_id} ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ")
        
    except Exception as e:
        import traceback
        error_detail = traceback.format_exc()
        logger.error(f"æ¨è«– {inference_id} ã®è©•ä¾¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}", exc_info=True)
        
        # ã‚¨ãƒ©ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
        error_message = str(e)
        error_type = e.__class__.__name__
        
        # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿå ´æ‰€ã®ç‰¹å®šã‚’è©¦ã¿ã‚‹
        error_context = ""
        try:
            tb = traceback.extract_tb(e.__traceback__)
            if tb:
                last_frame = tb[-1]
                error_context = f"{last_frame.filename}ã®{last_frame.name}é–¢æ•°({last_frame.lineno}è¡Œç›®)"
        except:
            pass
        
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯æ¨è«–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å¤±æ•—ã«æ›´æ–°ï¼ˆè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ä»˜ãï¼‰
        detailed_error = f"ã‚¨ãƒ©ãƒ¼: {error_type} - {error_message}"
        if error_context:
            detailed_error += f"\nå ´æ‰€: {error_context}"
        
        inference_repo.update_inference(inference_id, {
            "status": InferenceStatus.FAILED,
            "error": detailed_error
        })


@router.get("/{inference_id}", response_model=Inference)
async def get_inference(
    inference_id: str = Path(..., description="æ¨è«–ID")
):
    """
    ç‰¹å®šã®æ¨è«–ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        inference_id: æ¨è«–ID
        
    Returns:
        Inference: æ¨è«–æƒ…å ±
    """
    try:
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ã‚’å–å¾—
        inference_repo = get_inference_repository()
        inference_db = inference_repo.get_inference_by_id(inference_id)
        
        if not inference_db:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ¨è«–ID '{inference_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # çµæœã‚’å¤‰æ›
        results = []
        for res in inference_db.get("results", []):
            results.append(InferenceResult(
                id=res["id"],
                inference_id=res["inference_id"],
                input=res["input"],
                expected_output=res.get("expected_output"),
                actual_output=res["actual_output"],
                metrics=res.get("metrics"),
                latency=res.get("latency"),
                token_count=res.get("token_count"),
                created_at=datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
            ))
        
        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
        created_at = datetime.fromisoformat(inference_db["created_at"]) if isinstance(inference_db["created_at"], str) else inference_db["created_at"]
        updated_at = datetime.fromisoformat(inference_db["updated_at"]) if isinstance(inference_db["updated_at"], str) else inference_db["updated_at"]
        completed_at = None
        if inference_db.get("completed_at"):
            completed_at = datetime.fromisoformat(inference_db["completed_at"]) if isinstance(inference_db["completed_at"], str) else inference_db["completed_at"]
        
        # æ¨è«–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
        inference = Inference(
            id=inference_db["id"],
            name=inference_db["name"],
            description=inference_db.get("description"),
            dataset_id=inference_db["dataset_id"],
            provider_id=inference_db["provider_id"],
            model_id=inference_db["model_id"],
            status=InferenceStatus(inference_db["status"]),
            progress=inference_db["progress"],
            metrics=inference_db.get("metrics"),
            results=results,
            created_at=created_at,
            updated_at=updated_at,
            completed_at=completed_at,
            error=inference_db.get("error")
        )
        
        return inference
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨è«–å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


@router.put("/{inference_id}", response_model=Inference)
async def update_inference(
    inference_data: InferenceUpdate,
    inference_id: str = Path(..., description="æ¨è«–ID")
):
    """
    ç‰¹å®šã®æ¨è«–ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    
    Args:
        inference_id: æ›´æ–°ã™ã‚‹æ¨è«–ã®ID
        inference_data: æ›´æ–°ãƒ‡ãƒ¼ã‚¿
        
    Returns:
        Inference: æ›´æ–°ã•ã‚ŒãŸæ¨è«–æƒ…å ±
    """
    try:
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ã‚’å–å¾—
        inference_repo = get_inference_repository()
        inference_db = inference_repo.get_inference_by_id(inference_id)
        
        if not inference_db:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ¨è«–ID '{inference_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        update_data = inference_data.dict(exclude_unset=True)
        
        # æ›´æ–°ã‚’å®Ÿè¡Œ
        updated_inference = inference_repo.update_inference(inference_id, update_data)
        
        if not updated_inference:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="æ¨è«–ã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ"
            )
        
        # çµæœã‚’å¤‰æ›
        results = []
        for res in updated_inference.get("results", []):
            results.append(InferenceResult(
                id=res["id"],
                inference_id=res["inference_id"],
                input=res["input"],
                expected_output=res.get("expected_output"),
                actual_output=res["actual_output"],
                metrics=res.get("metrics"),
                latency=res.get("latency"),
                token_count=res.get("token_count"),
                created_at=datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
            ))
        
        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
        created_at = datetime.fromisoformat(updated_inference["created_at"]) if isinstance(updated_inference["created_at"], str) else updated_inference["created_at"]
        updated_at = datetime.fromisoformat(updated_inference["updated_at"]) if isinstance(updated_inference["updated_at"], str) else updated_inference["updated_at"]
        completed_at = None
        if updated_inference.get("completed_at"):
            completed_at = datetime.fromisoformat(updated_inference["completed_at"]) if isinstance(updated_inference["completed_at"], str) else updated_inference["completed_at"]
        
        # æ¨è«–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
        inference = Inference(
            id=updated_inference["id"],
            name=updated_inference["name"],
            description=updated_inference.get("description"),
            dataset_id=updated_inference["dataset_id"],
            provider_id=updated_inference["provider_id"],
            model_id=updated_inference["model_id"],
            status=InferenceStatus(updated_inference["status"]),
            progress=updated_inference["progress"],
            metrics=updated_inference.get("metrics"),
            results=results,
            created_at=created_at,
            updated_at=updated_at,
            completed_at=completed_at,
            error=updated_inference.get("error")
        )
        
        return inference
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨è«–æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–æ›´æ–°ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


@router.delete("/{inference_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_inference(
    inference_id: str = Path(..., description="æ¨è«–ID")
):
    """
    ç‰¹å®šã®æ¨è«–ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
    
    Args:
        inference_id: å‰Šé™¤ã™ã‚‹æ¨è«–ã®ID
    """
    try:
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ã‚’å–å¾—
        inference_repo = get_inference_repository()
        inference_db = inference_repo.get_inference_by_id(inference_id)
        
        if not inference_db:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ¨è«–ID '{inference_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # å‰Šé™¤ã‚’å®Ÿè¡Œ
        success = inference_repo.delete_inference(inference_id)
        
        if not success:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="æ¨è«–ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ"
            )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨è«–å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


@router.post("/{inference_id}/run", response_model=Inference)
async def run_inference(
    background_tasks: BackgroundTasks,
    inference_id: str = Path(..., description="æ¨è«–ID")
):
    """
    ç‰¹å®šã®æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
    
    Args:
        inference_id: å®Ÿè¡Œã™ã‚‹æ¨è«–ã®ID
        background_tasks: ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯
        
    Returns:
        Inference: å®Ÿè¡Œçµæœã®æ¨è«–æƒ…å ±
    """
    try:
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ã‚’å–å¾—
        inference_repo = get_inference_repository()
        inference_db = inference_repo.get_inference_by_id(inference_id)
        
        if not inference_db:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ¨è«–ID '{inference_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
        if inference_db["status"] == InferenceStatus.RUNNING:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="æ¨è«–ã¯æ—¢ã«å®Ÿè¡Œä¸­ã§ã™"
            )
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ã¨ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®å–å¾—
        provider_repo = get_provider_repository()
        model_repo = get_model_repository()
        
        provider = provider_repo.get_provider_by_id(inference_db["provider_id"])
        if not provider:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ãƒ—ãƒ­ãƒã‚¤ãƒ€ID '{inference_db['provider_id']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        model = model_repo.get_model_by_id(inference_db["model_id"])
        if not model:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"ãƒ¢ãƒ‡ãƒ«ID '{inference_db['model_id']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
        parameters = inference_db.get("parameters", {})
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’æŠ½å‡º
        dataset_name = inference_db["dataset_id"].split('/')[-1].replace('.json', '')
        
        # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’æ§‹ç¯‰
        model_config = ModelConfig(
            provider=provider["type"],
            model_name=model["name"],
            max_tokens=parameters.get("max_tokens", 512),
            temperature=parameters.get("temperature", 0.7),
            top_p=parameters.get("top_p", 1.0)
        )
        
        # è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ§‹ç¯‰
        evaluation_request = EvaluationRequest(
            datasets=[dataset_name],
            num_samples=parameters.get("num_samples", 100),
            n_shots=[parameters.get("n_shots", 0)],
            model=model_config
        )
        
        # æ¨è«–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        inference_repo.update_inference(inference_id, {
            "status": InferenceStatus.PENDING,
            "progress": 0,
            "error": None
        })
        
        # èƒŒæ™¯ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã‚’å®Ÿè¡Œ
        background_tasks.add_task(
            execute_inference_evaluation,
            inference_id=inference_id,
            evaluation_request=evaluation_request,
            provider_name=provider["type"],
            model_name=model["name"]
        )
        
        # æ›´æ–°ã•ã‚ŒãŸæ¨è«–ã‚’å–å¾—
        updated_inference = inference_repo.get_inference_by_id(inference_id)
        
        # çµæœã‚’å¤‰æ›
        results = []
        for res in updated_inference.get("results", []):
            results.append(InferenceResult(
                id=res["id"],
                inference_id=res["inference_id"],
                input=res["input"],
                expected_output=res.get("expected_output"),
                actual_output=res["actual_output"],
                metrics=res.get("metrics"),
                latency=res.get("latency"),
                token_count=res.get("token_count"),
                created_at=datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
            ))
        
        # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
        created_at = datetime.fromisoformat(updated_inference["created_at"]) if isinstance(updated_inference["created_at"], str) else updated_inference["created_at"]
        updated_at = datetime.fromisoformat(updated_inference["updated_at"]) if isinstance(updated_inference["updated_at"], str) else updated_inference["updated_at"]
        completed_at = None
        if updated_inference.get("completed_at"):
            completed_at = datetime.fromisoformat(updated_inference["completed_at"]) if isinstance(updated_inference["completed_at"], str) else updated_inference["completed_at"]
        
        # æ¨è«–ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ§‹ç¯‰
        inference = Inference(
            id=updated_inference["id"],
            name=updated_inference["name"],
            description=updated_inference.get("description"),
            dataset_id=updated_inference["dataset_id"],
            provider_id=updated_inference["provider_id"],
            model_id=updated_inference["model_id"],
            status=InferenceStatus(updated_inference["status"]),
            progress=updated_inference["progress"],
            metrics=updated_inference.get("metrics"),
            results=results,
            created_at=created_at,
            updated_at=updated_at,
            completed_at=completed_at,
            error=updated_inference.get("error")
        )
        
        return inference
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨è«–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}"
        )


@router.get("/{inference_id}/results", response_model=List[InferenceResult])
async def get_inference_results(
    inference_id: str = Path(..., description="æ¨è«–ID")
):
    """
    ç‰¹å®šã®æ¨è«–ã®çµæœä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        inference_id: æ¨è«–ID
        
    Returns:
        List[InferenceResult]: æ¨è«–çµæœä¸€è¦§
    """
    try:
        # ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰æ¨è«–ã‚’å–å¾—ï¼ˆå­˜åœ¨ãƒã‚§ãƒƒã‚¯ï¼‰
        inference_repo = get_inference_repository()
        inference_db = inference_repo.get_inference_by_id(inference_id)
        
        if not inference_db:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"æ¨è«–ID '{inference_id}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
            )
        
        # æ¨è«–çµæœã‚’å–å¾—
        results_db = inference_repo.get_inference_results(inference_id)
        
        # APIå¿œç­”ãƒ¢ãƒ‡ãƒ«ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        results = []
        for res in results_db:
            # æ—¥ä»˜æ–‡å­—åˆ—ã‚’datetimeã«å¤‰æ›
            created_at = datetime.fromisoformat(res["created_at"]) if isinstance(res["created_at"], str) else res["created_at"]
            
            results.append(InferenceResult(
                id=res["id"],
                inference_id=res["inference_id"],
                input=res["input"],
                expected_output=res.get("expected_output"),
                actual_output=res["actual_output"],
                metrics=res.get("metrics"),
                latency=res.get("latency"),
                token_count=res.get("token_count"),
                created_at=created_at
            ))
        
        return results
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¨è«–çµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æ¨è«–çµæœå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}"
        )