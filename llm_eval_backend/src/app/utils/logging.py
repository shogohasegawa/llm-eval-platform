import mlflow
from mlflow.entities import ViewType
from typing import Dict
import logging
import asyncio
import datetime
import time
import hashlib

from app.config.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

async def log_evaluation_results(model_name: str, metrics: Dict[str, float]) -> bool:
    """
    MLflowã«ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’ãƒ­ã‚°ã—ã¾ã™ã€‚
    éåŒæœŸé–¢æ•°ã¨ã—ã¦å®Ÿè£…ã—ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    åŒã˜ãƒ¢ãƒ‡ãƒ«åã®å ´åˆã¯æ—¢å­˜ã®Runã‚’æ›´æ–°ã—ã¾ã™ã€‚

    Args:
        model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆå½¢å¼: provider/modelï¼‰
        metrics: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸

    Returns:
        True: ãƒ­ã‚®ãƒ³ã‚°ãŒæˆåŠŸã—ãŸå ´åˆ
        False: ãƒ­ã‚®ãƒ³ã‚°ãŒå¤±æ•—ã—ãŸå ´åˆ
    """
    # Log a sample of metrics for debugging
    metrics_sample = list(metrics.items())[:5] if metrics else []
    logger.info(f"ğŸªµ log_evaluation_results called for {model_name} with {len(metrics)} metrics: {metrics_sample}")
    
    try:
        logger.info(f"ğŸªµ Logging to MLflow: {model_name} with metrics {list(metrics.keys())}")

        # Skip if metrics are empty
        if not metrics:
            logger.warning(f"MLflow logging skipped for {model_name} - No metrics to log")
            return False

        # Run MLflow operations synchronously in a separate thread
        def _do_mlflow_logging():
            try:
                # Set MLflow tracking URI
                if settings.MLFLOW_TRACKING_URI:
                    mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)
                    logger.info(f"MLflow tracking URI set to: {settings.MLFLOW_TRACKING_URI}")
                else:
                    logger.warning("MLflow tracking URI not set in settings, using default")
                
                # Log MLflow version and tracking URI for debugging
                logger.info(f"MLflow version: {mlflow.__version__}")
                logger.info(f"Current MLflow tracking URI: {mlflow.get_tracking_uri()}")
                
                # Create or get experiment
                experiment_name = "model_evaluation"
                mlflow.set_experiment(experiment_name)
                
                # Get experiment info
                experiment = mlflow.get_experiment_by_name(experiment_name)
                logger.info(f"Using experiment: {experiment_name} (ID: {experiment.experiment_id})")
                
                # æ—¢å­˜ã®ãƒ©ãƒ³ã‚’æ¤œç´¢ã™ã‚‹
                run_id = None
                try:
                    # MLflow ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                    from mlflow.client import MlflowClient
                    client = MlflowClient()
                    
                    # å¸¸ã«"base"ã‚¿ã‚°ãŒä»˜ã„ãŸè¦ªãƒ©ãƒ³ã‚’æ¤œç´¢ï¼ˆn_shotsã«é–¢ä¿‚ãªãåŒã˜ãƒ©ãƒ³ã‚’ä½¿ç”¨ï¼‰
                    # ãƒ¢ãƒ‡ãƒ«åã ã‘ã‚’æ¡ä»¶ã«ã™ã‚‹ã“ã¨ã§ã€ç•°ãªã‚‹n_shotsã§ã‚‚åŒã˜ãƒ©ãƒ³ã‚’ä½¿ã†
                    run_filter = f"params.model_name = '{model_name}' and tags.run_type = 'base'"
                    logger.info(f"Searching for existing base run with filter: {run_filter}")
                    matching_runs = client.search_runs(
                        experiment_ids=[experiment.experiment_id],
                        filter_string=run_filter,
                        max_results=1
                    )
                    
                    # åŒã˜ãƒ¢ãƒ‡ãƒ«åã§ãƒ©ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã®ãƒ©ãƒ³ã‚’å†åˆ©ç”¨
                    run_id = None
                    if matching_runs:
                        run_id = matching_runs[0].info.run_id
                        logger.info(f"Found existing run for {model_name} with ID: {run_id}, will update it")
                    
                except Exception as search_error:
                    logger.error(f"Error searching for runs: {str(search_error)}")
                    run_id = None
                
                try:
                    # æ—¢å­˜ã®ãƒ©ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ã€ãã®ãƒ©ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦æ›´æ–°
                    if run_id:
                        logger.info(f"Loading existing run with ID: {run_id}")
                        run = mlflow.start_run(run_id=run_id)
                    else:
                        # æ—¢å­˜ã®ãƒ©ãƒ³ãŒãªã„å ´åˆã¯æ–°ã—ã„ãƒ©ãƒ³ã‚’ä½œæˆ
                        logger.info(f"No existing run found for {model_name}, creating new run")
                        run = mlflow.start_run(run_name=model_name)
                        
                        # æ–°è¦ãƒ©ãƒ³ã®å ´åˆã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
                        mlflow.log_param("model_name", model_name)
                        mlflow.log_param("created_at", datetime.datetime.now().isoformat())
                        mlflow.log_param("model_type", model_name.split(':')[0] if ':' in model_name else model_name)
                        mlflow.log_param("supported_n_shots", "0,1,2,3,4,5")
                        
                        # æ–°è¦ãƒ©ãƒ³ã«å¿…ãšbaseã‚¿ã‚°ã‚’ä»˜ã‘ã‚‹
                        client.set_tag(run.info.run_id, "run_type", "base")
                        logger.info(f"âœ… Tagged new run as 'base'")
                    
                    run_id = run.info.run_id
                    logger.info(f"Run loaded/created with ID: {run_id}")
                    
                    # æ›´æ–°æ™‚åˆ»ã‚’ã‚¿ã‚°ã¨ã—ã¦ãƒ­ã‚°ï¼ˆã‚¿ã‚°ã¯ä½•åº¦ã§ã‚‚æ›´æ–°å¯èƒ½ï¼‰
                    try:
                        client.set_tag(run_id, "last_updated_at", datetime.datetime.now().isoformat())
                        logger.info(f"âœ… Updated 'last_updated_at' tag for run {run_id}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ Could not update 'last_updated_at' tag: {str(e)}")
                    
                    # n_shots å€¤ã‚’å–å¾—ï¼ˆã“ã‚Œã¯å¾Œã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚¹ãƒ†ãƒƒãƒ—å€¤ã¨ã—ã¦ä½¿ç”¨ï¼‰
                    n_shots_value = 0
                    if "n_shots_value" in metrics:
                        n_shots_value = metrics.pop("n_shots_value")
                        logger.info(f"Using n_shots value from metrics: {n_shots_value}")
                    else:
                        logger.warning("n_shots_value not found in metrics, defaulting to 0")
                    
                    # n_shots å€¤ã‚’ãƒ©ãƒ³ã®ã‚¿ã‚°ã¨ã—ã¦è¨˜éŒ²ï¼ˆè¡¨ç¤ºã§ã¯ä½¿ã‚ãªã„ãŒã€ãƒ‡ãƒãƒƒã‚°ã«ä¾¿åˆ©ï¼‰
                    client.set_tag(run_id, f"n_shots_{n_shots_value}_updated_at", datetime.datetime.now().isoformat())
                    
                    # Filter and convert metrics to numeric values
                    numeric_metrics = {}
                    for key, value in metrics.items():
                        if isinstance(value, (int, float)):
                            try:
                                numeric_metrics[key] = float(value)
                            except (ValueError, TypeError):
                                logger.warning(f"âš ï¸ Could not convert metric {key} to float: {value}")
                        else:
                            # Try to convert string numbers to float
                            if isinstance(value, str):
                                try:
                                    numeric_value = float(value)
                                    numeric_metrics[key] = numeric_value
                                    logger.info(f"ğŸ”„ Converted string metric {key} to float: {numeric_value}")
                                    continue
                                except (ValueError, TypeError):
                                    pass
                            logger.warning(f"âš ï¸ Skipping non-numeric metric {key}: {value} (type: {type(value).__name__})")
                    
                    # Log all metrics at once using log_metrics (more reliable than individual log_metric calls)
                    if numeric_metrics:
                        try:
                            formatted_metrics = dict(sorted(numeric_metrics.items()))
                            logger.info(f"ğŸ“Š Logging all metrics at once to MLflow: {list(formatted_metrics.keys())}")
                            
                            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚ˆã‚Šè©³ç´°ã«è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                            for metric_name, metric_value in formatted_metrics.items():
                                logger.info(f"ğŸ“Š MLflowè¨˜éŒ²äºˆå®šãƒ¡ãƒˆãƒªã‚¯ã‚¹: {metric_name} = {metric_value} (type: {type(metric_value).__name__})")
                            
                            # Get existing metrics to check for conflicts
                            existing_metrics = {}
                            try:
                                from mlflow.tracking.client import MlflowClient
                                client = MlflowClient()
                                run_data = client.get_run(run_id).data
                                for key, value in run_data.metrics.items():
                                    existing_metrics[key] = value
                                logger.info(f"Found {len(existing_metrics)} existing metrics in run {run_id}")
                            except Exception as e:
                                logger.warning(f"Could not retrieve existing metrics: {str(e)}")
                            
                            # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ n_shots_value ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ä½¿ç”¨ã—ã€çµ±ä¸€çš„ã«ãƒ­ã‚°è¨˜éŒ²
                            logger.info(f"Logging {len(formatted_metrics)} metrics with n_shots={n_shots_value} as step")
                            logged_count = 0
                            
                            # n_shots_value ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã¯ãªãã€ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã®ã§é™¤å¤–
                            if "n_shots_value" in formatted_metrics:
                                formatted_metrics.pop("n_shots_value")
                            
                            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åã‚’å®Œå…¨ã«æ­£è¦åŒ–ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                            cleaned_metrics = {}
                            for key, value in formatted_metrics.items():
                                logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åæ­£è¦åŒ–å‡¦ç†: '{key}'")
                                
                                # ã„ã£ãŸã‚“ "_" ã§åˆ†å‰²
                                parts = key.split("_")
                                # æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã—ã¦æ­£è¦åŒ–ã•ã‚ŒãŸå½¢å¼ã«æ§‹ç¯‰
                                
                                # 1. n_shotãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç‰¹å®š
                                shot_pattern = None
                                for part in parts:
                                    if part.endswith('shot'):
                                        shot_pattern = part
                                        break
                                
                                if shot_pattern:
                                    shot_num = shot_pattern.replace('shot', '')
                                    
                                    # 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åéƒ¨åˆ†ã‚’ç‰¹å®šï¼ˆæœ€å¾Œã®éƒ¨åˆ†ï¼‰
                                    metric_name = parts[-1]
                                    
                                    # 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’ç‰¹å®šï¼ˆæœ€åˆã®shotãƒ‘ã‚¿ãƒ¼ãƒ³å‰ã¾ã§ã®éƒ¨åˆ†ï¼‰
                                    dataset_parts = []
                                    for part in parts:
                                        if part.endswith('shot'):
                                            break
                                        dataset_parts.append(part)
                                    
                                    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåãŒãªã„å ´åˆã¯ãƒ€ãƒŸãƒ¼åã‚’ä½¿ç”¨
                                    dataset_name = "_".join(dataset_parts) if dataset_parts else "dataset"
                                    
                                    # 4. æ­£è¦åŒ–ã•ã‚ŒãŸå½¢å¼ã«å†æ§‹ç¯‰
                                    normalized_key = f"{dataset_name}_{shot_num}shot_{metric_name}"
                                    logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åæ­£è¦åŒ–: '{key}' â†’ '{normalized_key}'")
                                    
                                    cleaned_metrics[normalized_key] = value
                                else:
                                    # shotæƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã¯ãã®ã¾ã¾
                                    logger.info(f"ğŸ“Š shotæƒ…å ±ãªã—ã€ãã®ã¾ã¾ä½¿ç”¨: '{key}'")
                                    cleaned_metrics[key] = value
                            
                            # å…ƒã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç½®ãæ›ãˆ
                            formatted_metrics = cleaned_metrics
                            
                            # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åŒã˜n_shotså€¤ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦è¨˜éŒ²
                            for key, value in formatted_metrics.items():
                                try:
                                    mlflow.log_metric(key, value, step=n_shots_value)
                                    logger.info(f"âœ… Logged metric {key} = {value} with step={n_shots_value}")
                                    logged_count += 1
                                except Exception as e:
                                    logger.error(f"âŒ Failed to log metric {key}: {str(e)}")
                            
                            logger.info(f"âœ… Successfully logged {logged_count}/{len(formatted_metrics)} metrics to MLflow")
                        except Exception as metrics_error:
                            logger.error(f"âŒ Error logging metrics to MLflow: {str(metrics_error)}", exc_info=True)
                            # Fall back to logging metrics one by one
                            logger.info("ğŸ”„ Trying to log metrics one by one as fallback")
                            
                            # n_shots_value ã¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ã¯ãªãã€ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã®ã§é™¤å¤–
                            if "n_shots_value" in numeric_metrics:
                                numeric_metrics.pop("n_shots_value")
                            
                            logged_metrics_count = 0
                            for key, value in numeric_metrics.items():
                                try:
                                    # ã™ã¹ã¦ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’åŒã˜ã‚¹ãƒ†ãƒƒãƒ—ã§è¨˜éŒ²
                                    logger.info(f"ğŸ“Š å€‹åˆ¥ã«ãƒ­ã‚°: {key} = {value} with step={n_shots_value}")
                                    mlflow.log_metric(key, value, step=n_shots_value)
                                    logged_metrics_count += 1
                                except Exception as e:
                                    logger.error(f"âŒ Failed to log metric {key}: {str(e)}")
                            logger.info(f"âœ… Logged {logged_metrics_count}/{len(numeric_metrics)} metrics individually")
                    
                    logger.info(f"All MLflow logging operations completed")
                    
                    # Display MLflow UI URLs for easier debugging
                    tracking_uri = mlflow.get_tracking_uri()
                    if tracking_uri.startswith("http"):
                        logger.info(f"MLflow Run URL: {tracking_uri}/#/experiments/{experiment.experiment_id}/runs/{run_id}")
                        logger.info(f"MLflow Experiment URL: {tracking_uri}/#/experiments/{experiment.experiment_id}")
                    
                    # ãƒ©ãƒ³ã‚’çµ‚äº†
                    mlflow.end_run()
                    logger.info(f"MLflow logging completed for {model_name} with n_shots={n_shots_value}")
                    return True
                except Exception as run_error:
                    logger.error(f"Error during MLflow run: {str(run_error)}")
                    # Make sure to end the run
                    try:
                        mlflow.end_run()
                    except:
                        pass
                    return False
            except Exception as e:
                logger.error(f"Error in MLflow logging: {str(e)}")
                return False

        # Run the MLflow logging function in a separate thread
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _do_mlflow_logging)
        return result
    except Exception as e:
        logger.error(f"Exception in log_evaluation_results: {str(e)}")
        return False