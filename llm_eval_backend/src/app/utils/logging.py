import mlflow
from mlflow.entities import ViewType
from typing import Dict
import logging
import asyncio
import datetime
import time
import hashlib

from app.config.config import get_settings

settings = get_settings()
logger = logging.getLogger(__name__)

async def log_evaluation_results(model_name: str, metrics: Dict[str, float]) -> bool:
    """
    MLflowã«ãƒ¢ãƒ‡ãƒ«è©•ä¾¡çµæœã‚’ãƒ­ã‚°ã—ã¾ã™ã€‚
    éåŒæœŸé–¢æ•°ã¨ã—ã¦å®Ÿè£…ã—ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
    åŒã˜ãƒ¢ãƒ‡ãƒ«åã®å ´åˆã¯æ—¢å­˜ã®Runã‚’æ›´æ–°ã—ã¾ã™ã€‚

    Args:
        model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆå½¢å¼: provider/modelï¼‰
        metrics: è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¾æ›¸

    Returns:
        True: ãƒ­ã‚®ãƒ³ã‚°ãŒæˆåŠŸã—ãŸå ´åˆ
        False: ãƒ­ã‚®ãƒ³ã‚°ãŒå¤±æ•—ã—ãŸå ´åˆ
    """
    # Log a sample of metrics for debugging
    metrics_sample = list(metrics.items())[:5] if metrics else []
    logger.info(f"ğŸªµ log_evaluation_results called for {model_name} with {len(metrics)} metrics: {metrics_sample}")
    
    try:
        logger.info(f"ğŸªµ Logging to MLflow: {model_name} with metrics {list(metrics.keys())}")

        # Skip if metrics are empty
        if not metrics:
            logger.warning(f"MLflow logging skipped for {model_name} - No metrics to log")
            return False

        # Run MLflow operations synchronously in a separate thread
        def _do_mlflow_logging():
            try:
                # Set MLflow tracking URI
                if settings.MLFLOW_TRACKING_URI:
                    mlflow.set_tracking_uri(settings.MLFLOW_TRACKING_URI)
                    logger.info(f"MLflow tracking URI set to: {settings.MLFLOW_TRACKING_URI}")
                else:
                    logger.warning("MLflow tracking URI not set in settings, using default")
                
                # Log MLflow version and tracking URI for debugging
                logger.info(f"MLflow version: {mlflow.__version__}")
                logger.info(f"Current MLflow tracking URI: {mlflow.get_tracking_uri()}")
                
                # Create or get experiment
                experiment_name = "model_evaluation"
                mlflow.set_experiment(experiment_name)
                
                # Get experiment info
                experiment = mlflow.get_experiment_by_name(experiment_name)
                logger.info(f"Using experiment: {experiment_name} (ID: {experiment.experiment_id})")
                
                # æ—¢å­˜ã®ãƒ©ãƒ³ã‚’æ¤œç´¢ã™ã‚‹
                run_id = None
                try:
                    # MLflow ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨
                    from mlflow.client import MlflowClient
                    client = MlflowClient()
                    
                    # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦åŒã˜ãƒ¢ãƒ‡ãƒ«åã®ãƒ©ãƒ³ã‚’æ¤œç´¢
                    run_filter = f"params.model_name = '{model_name}'"
                    logger.info(f"Searching for existing runs with filter: {run_filter}")
                    matching_runs = client.search_runs(
                        experiment_ids=[experiment.experiment_id],
                        filter_string=run_filter
                    )
                    
                    # åŒã˜ãƒ¢ãƒ‡ãƒ«åã§ãƒ©ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€ãã®ãƒ©ãƒ³ã‚’å†åˆ©ç”¨
                    if matching_runs:
                        run_id = matching_runs[0].info.run_id
                        logger.info(f"Found existing run for {model_name} with ID: {run_id}, will update it")
                    
                except Exception as search_error:
                    logger.error(f"Error searching for runs: {str(search_error)}")
                    run_id = None
                
                try:
                    # æ—¢å­˜ã®ãƒ©ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã¯ã€ãã®ãƒ©ãƒ³ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦æ›´æ–°
                    if run_id:
                        logger.info(f"Loading existing run with ID: {run_id}")
                        run = mlflow.start_run(run_id=run_id)
                    else:
                        # æ—¢å­˜ã®ãƒ©ãƒ³ãŒãªã„å ´åˆã¯æ–°ã—ã„ãƒ©ãƒ³ã‚’ä½œæˆ
                        logger.info(f"No existing run found for {model_name}, creating new run")
                        run = mlflow.start_run(run_name=model_name)
                        
                        # æ–°è¦ãƒ©ãƒ³ã®å ´åˆã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°
                        mlflow.log_param("model_name", model_name)
                        mlflow.log_param("created_at", datetime.datetime.now().isoformat())
                    
                    run_id = run.info.run_id
                    logger.info(f"Run loaded/created with ID: {run_id}")
                    
                    # æ›´æ–°æ™‚åˆ»ã‚’ãƒ­ã‚°
                    mlflow.log_param("updated_at", datetime.datetime.now().isoformat())
                    
                    # Filter and convert metrics to numeric values
                    numeric_metrics = {}
                    for key, value in metrics.items():
                        if isinstance(value, (int, float)):
                            try:
                                numeric_metrics[key] = float(value)
                            except (ValueError, TypeError):
                                logger.warning(f"âš ï¸ Could not convert metric {key} to float: {value}")
                        else:
                            # Try to convert string numbers to float
                            if isinstance(value, str):
                                try:
                                    numeric_value = float(value)
                                    numeric_metrics[key] = numeric_value
                                    logger.info(f"ğŸ”„ Converted string metric {key} to float: {numeric_value}")
                                    continue
                                except (ValueError, TypeError):
                                    pass
                            logger.warning(f"âš ï¸ Skipping non-numeric metric {key}: {value} (type: {type(value).__name__})")
                    
                    # Log all metrics at once using log_metrics (more reliable than individual log_metric calls)
                    if numeric_metrics:
                        try:
                            logger.info(f"ğŸ“Š Logging all metrics at once to MLflow: {list(numeric_metrics.keys())}")
                            mlflow.log_metrics(numeric_metrics)
                            logger.info(f"âœ… Successfully logged {len(numeric_metrics)} metrics to MLflow")
                        except Exception as metrics_error:
                            logger.error(f"âŒ Error logging metrics to MLflow: {str(metrics_error)}", exc_info=True)
                            # Fall back to logging metrics one by one
                            logger.info("ğŸ”„ Trying to log metrics one by one as fallback")
                            logged_metrics_count = 0
                            for key, value in numeric_metrics.items():
                                try:
                                    mlflow.log_metric(key, value)
                                    logged_metrics_count += 1
                                except Exception as e:
                                    logger.error(f"âŒ Failed to log metric {key}: {str(e)}")
                            logger.info(f"âœ… Logged {logged_metrics_count}/{len(numeric_metrics)} metrics individually")
                    
                    logger.info(f"All MLflow logging operations completed")
                    
                    # Display MLflow UI URL for easier debugging
                    tracking_uri = mlflow.get_tracking_uri()
                    if tracking_uri.startswith("http"):
                        logger.info(f"MLflow UI URL: {tracking_uri}/#/experiments/{experiment.experiment_id}/runs/{run_id}")
                    
                    # End the run
                    mlflow.end_run()
                    logger.info(f"MLflow logging completed for {model_name}")
                    return True
                except Exception as run_error:
                    logger.error(f"Error during MLflow run: {str(run_error)}")
                    # Make sure to end the run
                    try:
                        mlflow.end_run()
                    except:
                        pass
                    return False
            except Exception as e:
                logger.error(f"Error in MLflow logging: {str(e)}")
                return False

        # Run the MLflow logging function in a separate thread
        loop = asyncio.get_event_loop()
        result = await loop.run_in_executor(None, _do_mlflow_logging)
        return result
    except Exception as e:
        logger.error(f"Exception in log_evaluation_results: {str(e)}")
        return False