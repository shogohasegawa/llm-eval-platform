"""
è©•ä¾¡ã‚¸ãƒ§ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã®è©•ä¾¡ã‚¸ãƒ§ãƒ–å‡¦ç†ã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""
import asyncio
import json
import logging
import time
import traceback
from typing import Dict, Any, Optional, List
import datetime

from app.api.models import EvaluationRequest, JobStatus, JobLogLevel
from app.utils.db.jobs import get_job_repository
from app.core.evaluation import run_multiple_evaluations
from app.utils.logging import log_evaluation_results
from app.utils.litellm_helper import get_provider_options

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š
logger = logging.getLogger(__name__)


class JobManager:
    """
    è©•ä¾¡ã‚¸ãƒ§ãƒ–ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹
    """
    _instance = None
    
    def __new__(cls, *args, **kwargs):
        """ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æä¾›"""
        if cls._instance is None:
            cls._instance = super(JobManager, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        """ã‚¸ãƒ§ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ã®åˆæœŸåŒ–"""
        if self._initialized:
            return
        
        self.job_repo = get_job_repository()
        self._initialized = True
    
    async def submit_job(self, request: EvaluationRequest) -> Dict[str, Any]:
        """
        è©•ä¾¡ã‚¸ãƒ§ãƒ–ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        
        Args:
            request: è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                
        Returns:
            ä½œæˆã•ã‚ŒãŸã‚¸ãƒ§ãƒ–æƒ…å ±
        """
        # ã‚¸ãƒ§ãƒ–ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä½œæˆ
        job = self.job_repo.create_job(request)
        
        # ãƒ­ã‚°ã‚’è¿½åŠ 
        self.job_repo.add_job_log(
            job_id=job["id"],
            log_level=JobLogLevel.INFO,
            message="ã‚¸ãƒ§ãƒ–ãŒã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ã•ã‚Œã¾ã—ãŸ"
        )
        
        # éåŒæœŸã§ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
        asyncio.create_task(self._run_job(job["id"], request))
        
        return job
    
    async def _run_job(self, job_id: str, request: EvaluationRequest):
        """
        éåŒæœŸã§ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œ
        
        Args:
            job_id: ã‚¸ãƒ§ãƒ–ID
            request: è©•ä¾¡ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        """
        try:
            # ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã€Œå®Ÿè¡Œä¸­ã€ã«æ›´æ–°
            self.job_repo.update_job_status(job_id, JobStatus.RUNNING)
            
            # ãƒ­ã‚°ã‚’è¿½åŠ 
            self.job_repo.add_job_log(
                job_id=job_id,
                log_level=JobLogLevel.INFO,
                message="ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œã‚’é–‹å§‹ã—ã¾ã—ãŸ"
            )
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã¨è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            provider_name = request.model.provider
            model_name = request.model.model_name
            
            # è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ã”ã¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨ï¼‰
            additional_params = get_provider_options(provider_name)
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é©ç”¨ï¼ˆå„ªå…ˆï¼‰
            if request.model.additional_params:
                # ãƒ˜ãƒƒãƒ€ãƒ¼ã®å ´åˆã¯æ›´æ–°
                if "headers" in additional_params and "headers" in request.model.additional_params:
                    additional_params["headers"].update(request.model.additional_params["headers"])
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å‰Šé™¤ï¼ˆé‡è¤‡é©ç”¨ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
                    user_params = request.model.additional_params.copy()
                    user_params.pop("headers", None)
                    additional_params.update(user_params)
                else:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒãªã„å ´åˆã¯ãã®ã¾ã¾æ›´æ–°
                    additional_params.update(request.model.additional_params)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå‡¦ç†é–‹å§‹ãƒ­ã‚°
            datasets_str = ", ".join(request.datasets)
            self.job_repo.add_job_log(
                job_id=job_id,
                log_level=JobLogLevel.INFO,
                message=f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {datasets_str} ã®è©•ä¾¡ã‚’é–‹å§‹ã—ã¾ã—ãŸ"
            )
            
            # è©•ä¾¡ã‚¨ãƒ³ã‚¸ãƒ³å‘¼ã³å‡ºã—
            results_full: Dict[str, Any] = await run_multiple_evaluations(
                datasets=request.datasets,
                provider_name=provider_name,
                model_name=model_name,
                num_samples=request.num_samples,
                n_shots=request.n_shots,
                additional_params=additional_params
            )
            
            # ãƒ•ãƒ©ãƒƒãƒˆãªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾æ›¸ã‚’ä½œæˆ
            flat_metrics: Dict[str, float] = {}
            for ds, ds_res in results_full.get("results", {}).items():
                details = ds_res.get("details", {})
                for key, value in details.items():
                    if key.endswith("_details") or key.endswith("_error_rate"):
                        continue
                    flat_metrics[key] = value  # ä¾‹: "aio_0shot_char_f1": 0.11
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®Œäº†ãƒ­ã‚°
                self.job_repo.add_job_log(
                    job_id=job_id,
                    log_level=JobLogLevel.INFO,
                    message=f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {ds} ã®è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸ"
                )
            
            # MLflowã¸ãƒ­ã‚°
            if flat_metrics and len(flat_metrics) > 0:
                try:
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ­ã‚° - ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’å‡ºåŠ›
                    metrics_sample = list(flat_metrics.items())[:5]
                    logger.info(f"ğŸ“Š ã‚¸ãƒ§ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼: MLflowã¸ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ­ã‚°ã‚’é–‹å§‹ã—ã¾ã™ - ãƒ¢ãƒ‡ãƒ«: {provider_name}/{model_name}")
                    logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ (5ä»¶): {metrics_sample}")
                    
                    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ç”¨ï¼‰
                    metrics_log_file = f"/app/job_metrics_{provider_name}_{model_name}_{int(time.time())}.json"
                    with open(metrics_log_file, "w") as f:
                        json.dump({
                            "provider": provider_name,
                            "model": model_name,
                            "timestamp": datetime.datetime.now().isoformat(),
                            "metrics": flat_metrics
                        }, f, indent=2)
                    logger.info(f"ğŸ“Š ã‚¸ãƒ§ãƒ–ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {metrics_log_file}")
                    
                    # MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°å®Ÿè¡Œ
                    logging_result = await log_evaluation_results(
                        model_name=f"{provider_name}/{model_name}",
                        metrics=flat_metrics
                    )
                    
                    # ãƒ­ã‚°çµæœã‚’ã‚¸ãƒ§ãƒ–ãƒ­ã‚°ã«è¨˜éŒ²
                    if logging_result:
                        self.job_repo.add_job_log(
                            job_id=job_id,
                            log_level=JobLogLevel.INFO,
                            message=f"MLflowã¸ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãƒ­ã‚®ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{len(flat_metrics)}å€‹ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰"
                        )
                        logger.info(f"âœ… MLflowãƒ­ã‚®ãƒ³ã‚°æˆåŠŸ: {provider_name}/{model_name}")
                    else:
                        self.job_repo.add_job_log(
                            job_id=job_id,
                            log_level=JobLogLevel.WARNING,
                            message="MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ"
                        )
                        logger.warning(f"âš ï¸ MLflowãƒ­ã‚®ãƒ³ã‚°å•é¡Œ: {provider_name}/{model_name}")
                except Exception as e:
                    # MLflowãƒ­ã‚®ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã¯ã‚¸ãƒ§ãƒ–å…¨ä½“ã‚’å¤±æ•—ã«ã¯ã—ãªã„
                    error_msg = str(e)
                    logger.error(f"âŒ MLflowãƒ­ã‚®ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {error_msg}")
                    
                    # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²
                    error_log_file = f"/app/job_mlflow_error_{provider_name}_{model_name}_{int(time.time())}.txt"
                    with open(error_log_file, "w") as f:
                        f.write(f"Error logging metrics for {provider_name}/{model_name}: {error_msg}\n\n")
                        import traceback
                        traceback.print_exc(file=f)
                    logger.error(f"âŒ ã‚¸ãƒ§ãƒ–ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã—ãŸ: {error_log_file}")
                    
                    self.job_repo.add_job_log(
                        job_id=job_id,
                        log_level=JobLogLevel.ERROR,
                        message=f"MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {error_msg}"
                    )
            else:
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒãªã„å ´åˆã¯ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ã ã‘
                logger.warning(f"MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“ - ãƒ¢ãƒ‡ãƒ«: {provider_name}/{model_name}")
                self.job_repo.add_job_log(
                    job_id=job_id,
                    log_level=JobLogLevel.WARNING,
                    message="ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒç©ºã®ãŸã‚MLflowã¸ã®ãƒ­ã‚®ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ"
                )
            
            # ã‚¸ãƒ§ãƒ–å®Œäº†ãƒ­ã‚°
            self.job_repo.add_job_log(
                job_id=job_id,
                log_level=JobLogLevel.INFO,
                message="ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ"
            )
            
            # çµæœã‚’ä¿å­˜ã—ã¦ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã€Œå®Œäº†ã€ã«æ›´æ–°
            response_data = {
                "model_info": request.model.dict(),
                "metrics": flat_metrics,
                "full_results": results_full
            }
            self.job_repo.update_job_status(
                job_id=job_id,
                status=JobStatus.COMPLETED,
                result_data=response_data
            )
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            error_msg = str(e)
            logger.error(f"ã‚¸ãƒ§ãƒ–å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (Job ID: {job_id}): {error_msg}")
            
            # ã‚¸ãƒ§ãƒ–ãƒ­ã‚°ã«ã‚¨ãƒ©ãƒ¼ã‚’è¿½åŠ 
            self.job_repo.add_job_log(
                job_id=job_id,
                log_level=JobLogLevel.ERROR,
                message=f"ã‚¸ãƒ§ãƒ–å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {error_msg}"
            )
            
            # ã‚¸ãƒ§ãƒ–ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã€Œå¤±æ•—ã€ã«æ›´æ–°
            self.job_repo.update_job_status(
                job_id=job_id,
                status=JobStatus.FAILED,
                error_message=error_msg
            )
    
    def get_job(self, job_id: str) -> Optional[Dict[str, Any]]:
        """
        ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’å–å¾—
        
        Args:
            job_id: ã‚¸ãƒ§ãƒ–ID
                
        Returns:
            ã‚¸ãƒ§ãƒ–æƒ…å ±ã¾ãŸã¯None
        """
        return self.job_repo.get_job_by_id(job_id)
    
    def get_all_jobs(self, page: int = 1, page_size: int = 10) -> Dict[str, Any]:
        """
        ã™ã¹ã¦ã®ã‚¸ãƒ§ãƒ–ã‚’å–å¾—
        
        Args:
            page: ãƒšãƒ¼ã‚¸ç•ªå·
            page_size: 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®ä»¶æ•°
                
        Returns:
            ã‚¸ãƒ§ãƒ–ãƒªã‚¹ãƒˆæƒ…å ±
        """
        jobs, total = self.job_repo.get_all_jobs(page, page_size)
        return {
            "jobs": jobs,
            "total": total,
            "page": page,
            "page_size": page_size
        }
    
    def get_job_logs(self, job_id: str) -> List[Dict[str, Any]]:
        """
        ã‚¸ãƒ§ãƒ–ã®ãƒ­ã‚°ã‚’å–å¾—
        
        Args:
            job_id: ã‚¸ãƒ§ãƒ–ID
                
        Returns:
            ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
        """
        return self.job_repo.get_job_logs(job_id)


# ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹é–¢æ•°
def get_job_manager() -> JobManager:
    """
    ã‚¸ãƒ§ãƒ–ãƒãƒãƒ¼ã‚¸ãƒ£ã®ã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
    
    Returns:
        JobManagerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    """
    return JobManager()
