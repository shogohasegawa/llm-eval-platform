version: '3.8'

# 単一サーバー用設定ファイル - すべてのサービスを1台のマシンで実行
# 使用方法: docker-compose up -d

services:
  api:
    build:
      context: ./llm_eval_backend
      dockerfile: Dockerfile
    env_file:
      - ./.env  # ルート直下の統合された環境変数ファイルを使用
    ports:
      - "${API_PORT:-8001}:8000"
    volumes:
      - ./llm_eval_backend/src:/app/src
      - ./llm_eval_backend/results:/app/results
      - ./datasets:/external_datasets
      - ./external_data:/external_data
    environment:
      - LLMEVAL_ENV=${LLMEVAL_ENV:-development}
      - LLMEVAL_LOG_LEVEL=${LLMEVAL_LOG_LEVEL:-INFO}
      - MLFLOW_TRACKING_URI=${MLFLOW_HOST_URI:-http://mlflow:5000}
      - MLFLOW_HOST=${MLFLOW_HOST:-mlflow}
      - MLFLOW_PORT=${MLFLOW_PORT:-5000}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - LLMEVAL_DB_PATH=${LLMEVAL_DB_PATH:-/external_data/llm_eval.db}
      - TZ=${TZ:-Asia/Tokyo}
    depends_on:
      - mlflow
      - ollama
    restart: unless-stopped
    networks:
      - llm-eval-network

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "${MLFLOW_EXTERNAL_PORT:-5000}:5000"
    volumes:
      - ./llm_eval_backend/mlflow:/mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=file:///mlflow/artifacts
      - MLFLOW_SERVE_ARTIFACTS=true
      - GUNICORN_CMD_ARGS="--access-logfile - --workers 1 --threads 2 --timeout 180 --forwarded-allow-ips='*' --log-level=debug"
      - ENABLE_CORS=true
    command: mlflow server --host 0.0.0.0 --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root file:///mlflow/artifacts --serve-artifacts
    user: root
    networks:
      - llm-eval-network
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_ORIGINS=${OLLAMA_ORIGINS:-*}
      - OLLAMA_MODELS_PATH=${OLLAMA_MODELS_PATH:-/root/.ollama}
      - NVIDIA_VISIBLE_DEVICES=all
    # Docker Composeバージョンに合わせたGPU設定
    runtime: nvidia
    networks:
      - llm-eval-network

  llm-leaderboard-app:
    build:
      context: ./llm_eval_frontend
      dockerfile: Dockerfile
    env_file:
      - ./.env  # ルート直下の統合された環境変数ファイルを使用
    ports:
      - "${FRONTEND_PORT:-4173}:3000"
    environment:
      # フロントエンド用環境変数
      - VITE_API_BASE_URL=${VITE_API_BASE_URL:-}
      - VITE_OLLAMA_BASE_URL=${VITE_OLLAMA_BASE_URL:-/ollama}
      - VITE_MLFLOW_BASE_URL=${VITE_MLFLOW_BASE_URL:-/mlflow}
      - VITE_API_TIMEOUT=${VITE_API_TIMEOUT:-60000}
      - VITE_DEBUG_MODE=${VITE_DEBUG_MODE:-false}
      - VITE_APP_NAME=${VITE_APP_NAME:-LLM評価プラットフォーム}
      - VITE_APP_VERSION=${VITE_APP_VERSION:-1.0.0}
      - VITE_LOG_LEVEL=${VITE_LOG_LEVEL:-INFO}
    depends_on:
      - api
    networks:
      - llm-eval-network
    restart: always

networks:
  llm-eval-network:
    driver: bridge

volumes:
  mlflow:
  ollama_data: