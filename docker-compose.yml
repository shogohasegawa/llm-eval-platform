version: '3.8'

services:
  api:
    build:
      context: ./llm_eval_backend
      dockerfile: Dockerfile
    env_file:
      - ./llm_eval_backend/.env
    ports:
      - "${API_PORT:-8001}:8000"
    volumes:
      - ./llm_eval_backend/src:/app/src
      - ./llm_eval_backend/results:/app/results
      - ./datasets:/external_datasets # 既存のデータセットディレクトリをマウント
      - ./external_data:/external_data # 既存のデータ保存用ディレクトリをマウント
    environment:
      - LLMEVAL_ENV=${LLMEVAL_ENV:-development}
      - LLMEVAL_LOG_LEVEL=${LLMEVAL_LOG_LEVEL:-INFO}
      - MLFLOW_TRACKING_URI=${MLFLOW_HOST_URI:-http://mlflow:5000}
      - MLFLOW_HOST=${MLFLOW_HOST:-mlflow}
      - MLFLOW_PORT=${MLFLOW_PORT:-5000}
      - LLMEVAL_DB_PATH=/external_data/llm_eval.db # 既存のDBファイルパス
      - TZ=Asia/Tokyo # タイムゾーン設定
    # APIサーバーはhealthエンドポイントを持っていない可能性があるため、ヘルスチェックを無効化
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:8000/health", "||", "exit", "1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 10s
    depends_on:
      - mlflow
    restart: unless-stopped
    networks:
      - llm-eval-network

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "${MLFLOW_EXTERNAL_PORT:-5000}:5000"
    volumes:
      - ./llm_eval_backend/mlflow:/mlflow # 既存のMLflowディレクトリをマウント
    environment:
      - MLFLOW_TRACKING_URI=http://0.0.0.0:5000
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=file:///mlflow/artifacts
      # クロスネットワークアクセス対応
      - MLFLOW_SERVE_ARTIFACTS=true
      - GUNICORN_CMD_ARGS="--access-logfile - --workers 1 --threads 2 --timeout 180 --forwarded-allow-ips='*' --log-level=debug"
      - ENABLE_CORS=true
    # アーティファクト提供を有効化
    command: mlflow server --host 0.0.0.0 --backend-store-uri sqlite:///mlflow/mlflow.db --default-artifact-root file:///mlflow/artifacts --serve-artifacts
    user: root # アーティファクトディレクトリのパーミッション問題解決用
    # MLflowサーバーのヘルスチェックを無効化
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:5000", "||", "exit", "1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 10s
    networks:
      - llm-eval-network

  # LLMサービス（Ollamaなど）と接続する場合の改善設定
  # Ollamaサービス - ローカルLLMモデルを実行（GPU対応版）
  ollama:
    profiles: ["${SKIP_OLLAMA:-default}"]  # SKIP_OLLAMA=true の場合は起動しない
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    # GPUサポート（古い形式を使用）
    runtime: nvidia
    networks:
      - llm-eval-network
    environment:
      # 環境変数で設定可能なパラメータ
      - OLLAMA_HOST=${OLLAMA_HOST:-0.0.0.0}
      - OLLAMA_MODELS_PATH=${OLLAMA_MODELS_PATH:-/root/.ollama}

  llm-leaderboard-app:
    build:
      context: ./llm_eval_frontend
      dockerfile: Dockerfile
    ports:
      - "${FRONTEND_PORT:-4173}:3000"
    # 環境ごとの設定が不要になるよう相対パスを使用
    # Viteのプロキシ設定で '/api' パスから APIサーバーへアクセス
    environment:
      # 環境変数で各サービスへの接続先を指定可能
      # 空の値を設定することで相対パスを使用するようになる
      - VITE_API_BASE_URL=${API_BASE_URL:-}
      # Ollamaサービスの接続先を環境変数で指定可能
      - VITE_OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-}
    # フロントエンドのヘルスチェックを無効化
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:3000", "||", "exit", "1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 15s
    depends_on:
      - api
    networks:
      - llm-eval-network
    restart: always

networks:
  llm-eval-network:
    driver: bridge

# データとモデルの永続化用ボリューム
volumes:
  ollama_data: