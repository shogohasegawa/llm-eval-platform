# LLM評価プラットフォーム統合環境変数設定ファイル
# ===========================================
# このファイルはシステム全体の環境変数を一元管理するためのテンプレートです
# このファイルをコピーして.envとして保存し、必要に応じて値を変更してください

#==============================
# 基本設定
#==============================
# 環境設定 (development, production)
# 用途: バックエンド動作モード設定、設定クラスで自動読み込み
LLMEVAL_ENV=production

# ログレベル (DEBUG, INFO, WARNING, ERROR)
# 用途: バックエンドログ設定、main.pyで読み込み
LLMEVAL_LOG_LEVEL=INFO

# データベースパス
# 用途: SQLiteデータベースの保存場所
LLMEVAL_DB_PATH=/external_data/llm_eval.db

# タイムゾーン設定
# 用途: コンテナ内のタイムゾーン設定
TZ=Asia/Tokyo

#==============================
# ネットワーク設定
#==============================
# APIサーバーポート (外部公開用)
# 用途: docker-compose.ymlでのポートマッピング
API_PORT=8001

# フロントエンドポート (外部公開用)
# 用途: docker-compose.ymlでのポートマッピング
FRONTEND_PORT=4173

# MLflowサーバー外部ポート
# 用途: docker-compose.ymlでのポートマッピング
MLFLOW_EXTERNAL_PORT=5000

# Ollamaサーバーポート
# 用途: docker-compose.ymlでのポートマッピング
OLLAMA_PORT=11434

#==============================
# MLflow設定
#==============================
# MLflowホスト名 (APIコンテナがMLflowに接続するホスト名)
# 用途: docker-compose.ymlでの接続、proxy.pyで使用
MLFLOW_HOST=mlflow

# MLflowポート (通常は変更不要)
# 用途: docker-compose.ymlでの接続、proxy.pyで使用
MLFLOW_PORT=5000

# MLflowホストURI (APIコンテナがMLflowに接続するURI)
# 用途: docker-compose.ymlでの接続、MLFLOW_TRACKING_URIにマップ
# proxy.pyでも使用
MLFLOW_HOST_URI=http://${MLFLOW_HOST}:${MLFLOW_PORT}

# 外部アクセス用MLflow URL（プロキシ用）
# 用途: 外部からのMLflowダッシュボードアクセス用
# ホスト名とポート番号を変数化して柔軟な設定が可能
MLFLOW_EXTERNAL_HOST=your-server-ip
LLMEVAL_MLFLOW_EXTERNAL_URI=http://${MLFLOW_EXTERNAL_HOST}:${MLFLOW_EXTERNAL_PORT}

#==============================
# Ollama設定
#==============================
# OllamaベースURL (APIコンテナがOllamaに接続するURL)
# 用途: docker-compose.ymlでの接続、proxy.pyで使用
OLLAMA_BASE_URL=http://ollama:11434

# Ollamaホスト (コンテナ内のバインドアドレス)
# 用途: ollamaコンテナの設定
OLLAMA_HOST=0.0.0.0

# Ollamaモデル保存パス
# 用途: ollamaコンテナのモデル保存先
OLLAMA_MODELS_PATH=/root/.ollama

# OllamaのCORS設定
# 用途: クロスオリジンリクエストの許可
OLLAMA_ORIGINS=*

# OllamaサーバーURL（分散デプロイメントの場合はGPUインスタンスのIPを指定）
# 用途: バックエンドからOllamaへの接続
# ホスト名とポート番号を変数化して柔軟な設定が可能
OLLAMA_EXTERNAL_HOST=your-server-ip
LLMEVAL_OLLAMA_BASE_URL=http://${OLLAMA_EXTERNAL_HOST}:${OLLAMA_PORT}

#==============================
# バックエンド固有の設定
#==============================
# データパス設定
# テスト用データセットの保存先
LLMEVAL_DATASET_DIR=/external_datasets/test/

# n-shot用データセットの保存先
LLMEVAL_TRAIN_DIR=/external_datasets/n_shot/

# 結果ディレクトリ
LLMEVAL_RESULTS_DIR=./results

# 最大トークン数
LLMEVAL_DEFAULT_MAX_TOKENS=1024

# 温度パラメータ
LLMEVAL_DEFAULT_TEMPERATURE=0.0

# Top-Pパラメータ
LLMEVAL_DEFAULT_TOP_P=1.0

# タイムアウト設定（秒）
LLMEVAL_MODEL_TIMEOUT=60.0

# 最大再試行回数
LLMEVAL_MODEL_RETRIES=3

# 再試行時の最小遅延（秒）
LLMEVAL_RETRY_BACKOFF_MIN=2.0

# 再試行時の最大遅延（秒）
LLMEVAL_RETRY_BACKOFF_MAX=30.0

# 再試行時の遅延倍率
LLMEVAL_RETRY_BACKOFF_MULTIPLIER=1.5

# LiteLLMキャッシュの有効化
LLMEVAL_ENABLE_LITELLM_CACHE=false

# キャッシュの有効期限（秒）
LLMEVAL_CACHE_EXPIRATION=3600

# バッチサイズ
LLMEVAL_BATCH_SIZE=10

# サンプル数のデフォルト値
LLMEVAL_DEFAULT_NUM_SAMPLES=10

# n-shotのデフォルト値
LLMEVAL_DEFAULT_N_SHOTS=[0, 2]

#==============================
# フロントエンド設定
#==============================
# APIサーバーのURL
# 用途: フロントエンドからAPIへの接続設定
# 空白の場合は相対パスを使用（プロキシ経由で接続）
VITE_API_BASE_URL=

# OllamaサーバーのURL
# 用途: フロントエンドからOllamaへの接続設定
# プロキシ経由で接続する場合は「/ollama」と設定
VITE_OLLAMA_BASE_URL=/ollama

# フロントエンド用MLflow設定
# 用途: フロントエンドからMLflowへの接続設定
# ブラウザから直接アクセスするためのURL
VITE_MLFLOW_DIRECT_URL=http://localhost:5001
# API経由でのMLflowへのプロキシパス
VITE_MLFLOW_PROXY_ENDPOINT=/proxy-mlflow
# MLflow状態確認用のエンドポイント
VITE_MLFLOW_STATUS_ENDPOINT=/mlflow-status
# MLflowのコンテナ内ホスト名とポート（バックエンド用）
VITE_MLFLOW_INTERNAL_HOST=llm-mlflow-tracking
VITE_MLFLOW_INTERNAL_PORT=5000

# フロントエンド用Ollama設定
# 用途: フロントエンドからOllamaへの接続設定
# Ollamaサーバーへの接続方法指定（以下のいずれかを選択）
# - プロキシ経由でアクセス（推奨・デフォルト）: VITE_OLLAMA_BASE_URL=/proxy-ollama
# - 直接アクセス（開発環境）: VITE_OLLAMA_BASE_URL=http://localhost:11434
# - 外部サーバーアクセス: VITE_OLLAMA_BASE_URL=http://192.168.3.43:11434
VITE_OLLAMA_BASE_URL=/proxy-ollama

# APIリクエストのタイムアウト設定（ミリ秒）
# 用途: API呼び出しの最大待機時間
VITE_API_TIMEOUT=60000

# デバッグモード（開発環境では true に設定）
# 用途: 詳細なログ出力とデバッグ情報の表示
VITE_DEBUG_MODE=false

# 開発モード特別設定
# 用途: 開発環境での動作設定
# API接続チェックをスキップするかどうか
VITE_DEV_SKIP_API_CHECK=true
# MLflow接続を常に成功とみなすかどうか
VITE_DEV_FORCE_MLFLOW_OK=true

# アプリケーション名
# 用途: UI表示用のアプリケーション名
VITE_APP_NAME=LLM評価プラットフォーム

# アプリケーションバージョン
# 用途: UI表示用のバージョン情報
VITE_APP_VERSION=1.0.0

# ログレベル（DEBUG, INFO, WARN, ERROR）
# 用途: フロントエンドのログ出力レベル設定
VITE_LOG_LEVEL=INFO

#==============================
# オプション設定
#==============================
# CORS設定 (カンマ区切りで複数指定可)
# 用途: バックエンドのCORS設定
# 本番環境では特定のドメインに限定することを推奨
CORS_ORIGINS=*

# Ollamaスキップフラグ (true=Ollamaを起動しない)
# 用途: docker-compose.ymlでの条件付き起動
SKIP_OLLAMA=false

# GPU設定 (GPUが利用可能な場合)
# 用途: GPUリソース割り当て
GPU_COUNT=1