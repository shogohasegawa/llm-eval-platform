# LLM評価プラットフォーム - ネットワーク構造解説

このドキュメントでは、LLM評価プラットフォームのネットワーク構造について詳細に解説します。各コンポーネントの役割、通信方法、およびシステム全体の接続関係を把握することで、プラットフォームの理解と効果的な運用が可能になります。

## 1. アーキテクチャ概要

LLM評価プラットフォームは、以下の主要コンポーネントから構成されています：

| コンポーネント名 | 説明 | 技術スタック | ポート設定 |
|----------------|------|-------------|-----------|
| llm-frontend-app | ユーザーインターフェース | React | 3000 (内部) → ${FRONTEND_PORT} (外部) |
| llm-api-backend | APIサーバー | FastAPI | 8000 (内部) → ${API_PORT} (外部) |
| llm-mlflow-tracking | メトリクス追跡・ダッシュボード | MLflow | 5000 (内部) → 5001 (外部) |
| 外部Ollama | モデル実行環境 | Ollama | ${OLLAMA_BASE_URL} (外部サービス) |

すべてのコンポーネントは共通の `llm-eval-network` (Docker Bridge Network) 上で通信し、コンテナ間のシームレスな接続を実現しています。

## 2. コンポーネント詳細

### 2.1 フロントエンドアプリケーション (llm-frontend-app)

Reactベースのフロントエンドアプリケーションで、ユーザーがプラットフォームと対話するための主要なインターフェースを提供します。

**主な特徴:**
- React Router を使用したSPAナビゲーション
- React Query によるデータフェッチ最適化
- バックエンドAPIへの通信クライアント
- プロキシを介したMLflowおよびOllamaへのアクセス

**通信設定:**
- バックエンドAPI: `VITE_API_BASE_URL` (空の場合は相対パス使用)
- MLflow直接URL: `VITE_MLFLOW_DIRECT_URL`
- Ollamaベース: `VITE_OLLAMA_BASE_URL`

### 2.2 APIバックエンド (llm-api-backend)

FastAPIベースのバックエンドサービスで、RESTful APIエンドポイントを提供するとともに、MLflowとOllamaへのプロキシ機能も担います。

**主な特徴:**
- RESTful API (`/api/v1/`) - データセット、モデル、評価結果の管理
- MLflowプロキシ (`/proxy-mlflow/`) - フロントエンドとMLflow間の橋渡し
- Ollamaプロキシ (`/proxy-ollama/`) - フロントエンドとOllama間の橋渡し
- CORS対応による柔軟なクロスオリジンリクエスト

**プロキシの詳細実装:**
- 複数接続先試行のフォールバックメカニズム
- コンテンツ変換 (URL書き換え、パス調整)
- エラーハンドリングと診断情報の提供

### 2.3 MLflowトラッキングサーバー (llm-mlflow-tracking)

MLflowを使用したメトリクス追跡と実験管理サーバーで、評価結果の保存と可視化を担当します。

**主な特徴:**
- 実験管理と追跡
- メトリクスの記録と比較
- アーティファクト保存 (チャート、結果ファイル)
- RESTful API および Web UI

**通信設定:**
- SQLiteバックエンド: `MLFLOW_BACKEND_STORE_URI`
- アーティファクト保存先: `MLFLOW_DEFAULT_ARTIFACT_ROOT`
- CORS設定: `MLFLOW_CORS_ALLOW_ALL_ORIGINS` など

### 2.4 外部Ollamaサービス

外部サーバーで動作するOllamaサービスで、LLMモデルの実行環境を提供します。  
（※現在の構成では、Docker Composeファイル内でのOllamaコンテナはコメントアウトされ、外部サービスを使用）

**主な特徴:**
- ローカルLLMモデルのホスティング
- モデル管理と推論API
- シンプルなRESTful API

**通信設定:**
- ベースURL: `OLLAMA_BASE_URL`
- CORS設定: `OLLAMA_ORIGINS`

## 3. ネットワークフロー詳細

### 3.1 ユーザーアクセスフロー

1. **ユーザー → フロントエンド**:  
   ポート `${FRONTEND_PORT}` (デフォルト: 4173) を通じてWebUIにアクセス

2. **ユーザー → バックエンドAPI**:  
   ポート `${API_PORT}` (デフォルト: 8001) を通じて直接APIにアクセス可能（高度ユーザー向け）

### 3.2 内部通信フロー

#### フロントエンド-バックエンド間通信

1. **APIリクエスト**:  
   フロントエンドはバックエンドAPIに HTTP リクエストを送信（`/api/v1/` エンドポイント）

2. **MLflowアクセス**:  
   フロントエンドは `/proxy-mlflow/` パスを通じてバックエンドに接続し、バックエンドはプロキシとして MLflow へリクエストを転送

3. **Ollamaアクセス**:  
   フロントエンドは `/proxy-ollama/` パスを通じてバックエンドに接続し、バックエンドはプロキシとして Ollama へリクエストを転送

#### バックエンド-MLflow間通信

1. **メトリクス保存・取得**:  
   バックエンドは環境変数 `MLFLOW_HOST` と `MLFLOW_PORT` を使用して MLflow サーバーと通信

2. **フォールバックメカニズム**:  
   複数の接続先URLを試行する堅牢な設計
   ```
   優先順位:
   1. 環境変数指定のホスト (デフォルト: mlflow:5000)
   2. ローカルホスト (同一コンテナ内接続)
   3. Docker内部ネットワークアドレス
   4. Bridge ネットワークゲートウェイ
   ```

#### バックエンド-Ollama間通信

1. **モデル実行リクエスト**:  
   バックエンドは環境変数 `OLLAMA_BASE_URL` を使用して Ollama サーバーと通信

2. **フォールバックメカニズム**:  
   MLflow同様、複数の接続先URLを試行
   ```
   優先順位:
   1. 環境変数指定のURL (デフォルト: http://ollama:11434)
   2. ローカルホスト (同一コンテナ内接続)
   3. Docker内部ネットワークアドレス
   ```

### 3.3 プロキシ機能詳細

#### MLflowプロキシ機能

バックエンドの `/proxy-mlflow/` エンドポイントは、以下の処理を行います：

1. **リクエスト転送**:  
   フロントエンドからのリクエストを MLflow サーバーに転送

2. **レスポンス変換**:  
   - HTML内のリンクとパス書き換え
   - CSSとJavaScriptのURL修正
   - JSONレスポンス内のパス調整
   - アーティファクトURIの変換

3. **エラー処理**:  
   - 接続エラー時のフォールバック
   - ユーザーフレンドリーなエラーメッセージ
   - タイムアウト処理

4. **静的ファイル対応**:  
   `/proxy-mlflow/static-files/` パスで MLflow の静的アセットを提供

#### Ollamaプロキシ機能

バックエンドの `/proxy-ollama/` エンドポイントは、以下の処理を行います：

1. **リクエスト転送**:  
   フロントエンドからのリクエストを Ollama サーバーに転送

2. **CORSヘッダー追加**:  
   クロスオリジン通信のためのヘッダー設定

3. **エラー処理**:  
   - 接続エラー時のフォールバック
   - 診断情報の提供

## 4. ボリューム (データ共有)

プラットフォームでは以下のボリュームを使用してデータ共有を行っています：

| ボリューム名 | マウントパス | 用途 |
|------------|------------|------|
| MLflow | ./llm_eval_backend/mlflow:/mlflow | MLflowのDB・アーティファクト保存 |
| llm_eval_backend/results | ./llm_eval_backend/results:/app/results | 評価結果出力の保存 |
| datasets | ./datasets:/external_datasets | テストデータセットの共有 |
| external_data | ./external_data:/external_data | SQLite DBなど外部データ保存 |

## 5. 環境変数とネットワーク設定

システムのネットワーク動作は環境変数によって柔軟に設定可能です。主要なネットワーク関連環境変数を以下に示します：

### ポート設定
- `API_PORT`: バックエンドAPIの外部公開ポート (デフォルト: 8001)
- `FRONTEND_PORT`: フロントエンドの外部公開ポート (デフォルト: 4173)
- `MLFLOW_EXTERNAL_PORT`: MLflowの外部公開ポート (デフォルト: 5000)
- `OLLAMA_PORT`: Ollamaの外部公開ポート (デフォルト: 11434)

### ホスト設定
- `MLFLOW_HOST`: MLflowサーバーのホスト名 (デフォルト: llm-mlflow-tracking)
- `MLFLOW_EXTERNAL_HOST`: 外部からアクセス可能なMLflowホスト
- `OLLAMA_EXTERNAL_HOST`: 外部からアクセス可能なOllamaホスト

### URL設定
- `VITE_API_BASE_URL`: フロントエンドが使用するAPIベースURL
- `VITE_OLLAMA_BASE_URL`: フロントエンドが使用するOllamaベースURL
- `OLLAMA_BASE_URL`: バックエンドが使用するOllamaベースURL
- `VITE_MLFLOW_DIRECT_URL`: フロントエンドからMLflowへの直接アクセスURL

### CORS設定
- `CORS_ORIGINS`: APIバックエンドのCORS許可オリジン (デフォルト: *)
- `OLLAMA_ORIGINS`: OllamaのCORS許可オリジン (デフォルト: *)

## 6. 障害対策と耐性

プラットフォームには以下の障害対策機能が実装されています：

1. **複数接続先試行**:  
   バックエンドのプロキシ機能は複数の接続先URLを順番に試行し、障害時のフェイルオーバーを実現

2. **タイムアウト設定**:  
   各リクエストには適切なタイムアウト値が設定され、長時間の応答待ちを防止

3. **エラーハンドリング**:  
   詳細なエラー情報の提供と適切なステータスコードの返却

4. **診断エンドポイント**:  
   `/debug-mlflow` などの診断用エンドポイントによる接続状態の確認機能

## 7. セキュリティ考慮事項

現在のネットワーク構成におけるセキュリティポイント：

1. **内部ネットワーク**:  
   コンポーネント間の通信は `llm-eval-network` 内に限定され、直接外部からアクセスできない

2. **CORS設定**:  
   クロスオリジンリクエストの制御（現在はすべてのオリジンを許可）

3. **ポートマッピング**:  
   必要なサービスのみを外部に公開し、内部サービスは直接アクセスできない

### セキュリティ向上のための推奨事項

- 本番環境では `CORS_ORIGINS` を特定のドメインに制限
- 必要な場合のみ外部からアクセス可能なポートを公開
- APIキーや認証機構の追加検討
- プロキシサーバー経由でのアクセス制限

## 8. トラブルシューティング

一般的なネットワーク問題と解決方法：

### 接続エラー

1. **フロントエンド-バックエンド間接続エラー**:
   - 環境変数 `VITE_API_BASE_URL` が正しく設定されているか確認
   - バックエンドサービスが起動しているか確認
   - ネットワークポートが正しくマッピングされているか確認

2. **バックエンド-MLflow間接続エラー**:
   - 環境変数 `MLFLOW_HOST` と `MLFLOW_PORT` が正しく設定されているか確認
   - MLflowサービスが起動しているか確認
   - `/debug-mlflow` エンドポイントでMLflow接続状態を診断

3. **バックエンド-Ollama間接続エラー**:
   - 環境変数 `OLLAMA_BASE_URL` が正しく設定されているか確認
   - 外部Ollamaサービスが実行中か確認
   - Ollamaサービスが適切なポートでリッスンしているか確認

### プロキシ問題

1. **MLflowプロキシエラー**:
   - ブラウザコンソールでCORSエラーがないか確認
   - ネットワーク構成がコンテナ間通信を許可しているか確認
   - MLflowのログで詳細エラーを確認

2. **Ollamaプロキシエラー**:
   - Ollamaサービスが起動しているか確認
   - CORS設定が適切か確認
   - API応答時間が長い場合はタイムアウト設定を調整

## 9. まとめ

LLM評価プラットフォームのネットワーク構造は、フロントエンド、バックエンド、MLflow、Ollamaの各コンポーネントが連携し、ユーザーに統合されたエクスペリエンスを提供します。プロキシ機能を活用した柔軟なアーキテクチャと、フォールバックメカニズムによる障害耐性を備えており、環境変数による柔軟な設定が可能です。

このドキュメントで説明したネットワーク構造を理解することで、プラットフォームの効率的な運用とカスタマイズが可能になります。