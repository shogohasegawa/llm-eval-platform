# LLM評価プラットフォーム - ネットワーク構造解説

このドキュメントでは、LLM評価プラットフォームのネットワーク構造について詳細に解説します。各コンポーネントの役割、通信方法、およびシステム全体の接続関係を把握することで、プラットフォームの理解と効果的な運用が可能になります。

## 1. アーキテクチャ概要

LLM評価プラットフォームは、以下の主要コンポーネントから構成されています：

| コンポーネント名 | 説明 | 技術スタック | ポート設定 |
|----------------|------|-------------|-----------|
| llm-frontend-app | ユーザーインターフェース | React | 3000 (内部) → ${FRONTEND_PORT} (外部) |
| llm-api-backend | APIサーバー | FastAPI | 8000 (内部) → ${API_PORT} (外部) |
| llm-mlflow-tracking | メトリクス追跡・ダッシュボード | MLflow | 5000 (内部) → ${MLFLOW_EXTERNAL_PORT} (外部) |
| 外部Ollama | モデル実行環境 | Ollama | 外部サービスとして設定 |

すべてのコンポーネントは共通の `llm-eval-network` (Docker Bridge Network) 上で通信し、コンテナ間のシームレスな接続を実現しています。

## 2. コンポーネント詳細

### 2.1 フロントエンドアプリケーション (llm-frontend-app)

Reactベースのフロントエンドアプリケーションで、ユーザーがプラットフォームと対話するための主要なインターフェースを提供します。

**主な特徴:**
- React Router を使用したSPAナビゲーション
- React Query によるデータフェッチ最適化
- バックエンドAPIへの通信クライアント
- プロキシを介したMLflowおよびOllamaへのアクセス

**通信設定:**
- バックエンドAPI: `VITE_API_BASE_URL` (空の場合は相対パス使用)
- MLflow直接URL: `VITE_MLFLOW_DIRECT_URL` (MLFLOW_EXTERNAL_URIから設定)

### 2.2 APIバックエンド (llm-api-backend)

FastAPIベースのバックエンドサービスで、RESTful APIエンドポイントを提供するとともに、MLflowとOllamaへのプロキシ機能も担います。

**主な特徴:**
- RESTful API (`/api/v1/`) - データセット、モデル、評価結果の管理
- MLflowプロキシ (`/proxy-mlflow/`) - フロントエンドとMLflow間の橋渡し
- Ollamaプロキシ (`/proxy-ollama/`) - フロントエンドとOllama間の橋渡し
- CORS対応による柔軟なクロスオリジンリクエスト

**プロキシの詳細実装:**
- 複数接続先試行のフォールバックメカニズム
- コンテンツ変換 (URL書き換え、パス調整)
- エラーハンドリングと診断情報の提供

### 2.3 MLflowトラッキングサーバー (llm-mlflow-tracking)

MLflowを使用したメトリクス追跡と実験管理サーバーで、評価結果の保存と可視化を担当します。

**主な特徴:**
- 実験管理と追跡
- メトリクスの記録と比較
- アーティファクト保存 (チャート、結果ファイル)
- RESTful API および Web UI

**通信設定:**
- SQLiteバックエンド: `MLFLOW_BACKEND_STORE_URI`
- アーティファクト保存先: `MLFLOW_DEFAULT_ARTIFACT_ROOT`
- CORS設定: `MLFLOW_CORS_ALLOW_ALL_ORIGINS` など

### 2.4 外部Ollamaサービス

外部サーバーで動作するOllamaサービスで、LLMモデルの実行環境を提供します。
（※Docker Composeファイル内でのOllamaコンテナはコメントアウトされており、外部サービスを使用）

**主な特徴:**
- ローカルLLMモデルのホスティング
- モデル管理と推論API
- シンプルなRESTful API

## 3. ネットワークフロー詳細

### 3.1 ユーザーアクセスフロー

1. **ユーザー → フロントエンド**:  
   ポート `${FRONTEND_PORT}` (デフォルト: 4173) を通じてWebUIにアクセス

2. **ユーザー → バックエンドAPI**:  
   ポート `${API_PORT}` (デフォルト: 8001) を通じて直接APIにアクセス可能（高度ユーザー向け）

3. **ユーザー → MLflowダッシュボード**:  
   ポート `${MLFLOW_EXTERNAL_PORT}` (デフォルト: 5001) を通じて直接MLflowにアクセス可能（開発/分析者向け）

### 3.2 内部通信フロー

#### フロントエンド-バックエンド間通信

1. **APIリクエスト**:  
   フロントエンドはバックエンドAPIに HTTP リクエストを送信（`/api/v1/` エンドポイント）

2. **MLflowアクセス**:
   以下の2つの方法が利用可能（直接アクセスが優先されます）：
   - 直接アクセス: `VITE_MLFLOW_DIRECT_URL` (MLFLOW_EXTERNAL_URI) を使用して MLflow APIエンドポイントに直接接続 `/api/2.0/mlflow/experiments/list` など
   - プロキシ経由: `/proxy-mlflow/` パスを通じてバックエンドに接続し、バックエンドはプロキシとして MLflow へリクエストを転送

3. **Ollamaアクセス**:
   フロントエンドはバックエンドの `/api/v1/ollama/` APIエンドポイントを使用してOllamaと通信
   - バックエンドが内部的にOllamaと通信し、結果をフロントエンドに返す
   - フロントエンドからの直接アクセスは使用されていない
   - 注意: `/proxy-ollama/` プロキシ機能は使用されておらず、削除されました (2025/5/13)

#### バックエンド-MLflow間通信

1. **メトリクス保存・取得**:  
   バックエンドはMLflowサーバーと通信する際、以下の優先順位でベースURLを決定:

   ```
   優先順位:
   1. MLFLOW_HOST_URI (内部接続用URI)
   2. "http://llm-mlflow-tracking:5000" (Dockerネットワーク内コンテナ名)
   3. MLFLOW_EXTERNAL_URI (全システム共通の外部アクセスURL)
   4. "http://localhost:5001" (ローカル開発フォールバック)
   
   # 後方互換性
   5. LLMEVAL_MLFLOW_EXTERNAL_URI (旧環境変数、非推奨)
   ```

2. **フォールバックメカニズム**:  
   - 最初のURLで接続できない場合、次のURLを試行
   - すべてのURLで接続失敗した場合、明示的なエラーを返却

#### バックエンド-Ollama間通信

1. **モデル実行リクエスト**:  
   バックエンドは、設定されたOllama接続情報を使用してOllamaサービスと通信

### 3.3 プロキシ機能詳細

#### MLflowプロキシ機能

バックエンドの `/proxy-mlflow/` エンドポイントは、以下の処理を行います：

1. **リクエスト転送**:
   フロントエンドからのリクエストを MLflow サーバーに転送

2. **レスポンス変換**:
   - HTML内のリンクとパス書き換え
   - CSSとJavaScriptのURL修正
   - JSONレスポンス内のパス調整
   - アーティファクトURIの変換

3. **エラー処理**:
   - 接続エラー時のフォールバック
   - ユーザーフレンドリーなエラーメッセージ
   - タイムアウト処理

4. **静的ファイル対応**:
   `/proxy-mlflow/static-files/` パスで MLflow の静的アセットを提供

#### Ollamaプロキシ機能（削除済み）

バックエンドの `/proxy-ollama/` エンドポイントは**2025年5月13日に削除されました**。代わりにフロントエンドはバックエンドの`/api/v1/ollama/`APIエンドポイントを使用します。

#### Ollama APIエンドポイント (実際に使用されている機能)

バックエンドの `/api/v1/ollama/` エンドポイントは、以下の機能を提供します：

1. **モデル管理API**:
   - モデルダウンロード開始 (`/api/v1/ollama/download`)
   - ダウンロード状態確認 (`/api/v1/ollama/download/{id}`)
   - 全ダウンロード一覧取得 (`/api/v1/ollama/downloads`)
   - モデル存在確認 (`/api/v1/ollama/check_model`)

2. **安全なアクセス**:
   - バックエンドがOllamaサーバーとの通信を担当
   - APIキーやセキュリティを一元管理
   - フロントエンドに必要な情報のみを提供

## 4. ボリューム (データ共有)

プラットフォームでは以下のボリュームを使用してデータ共有を行っています：

| ボリューム名 | マウントパス | 用途 |
|------------|------------|------|
| MLflow | ./llm_eval_backend/mlflow:/mlflow | MLflowのDB・アーティファクト保存 |
| llm_eval_backend/results | ./llm_eval_backend/results:/app/results | 評価結果出力の保存 |
| datasets | ./datasets:/external_datasets | テストデータセットの共有 |
| external_data | ./external_data:/external_data | SQLite DBなど外部データ保存 |

## 5. 環境変数とネットワーク設定

システムのネットワーク動作は環境変数によって柔軟に設定可能です。主要なネットワーク関連環境変数を以下に示します：

### ポート設定
- `API_PORT`: バックエンドAPIの外部公開ポート (デフォルト: 8001)
- `FRONTEND_PORT`: フロントエンドの外部公開ポート (デフォルト: 4173)
- `MLFLOW_EXTERNAL_PORT`: MLflowの外部公開ポート (デフォルト: 5001)

### ホスト設定
- `MLFLOW_HOST`: MLflowサーバーのホスト名 (デフォルト: llm-mlflow-tracking)
- `MLFLOW_PORT`: MLflowサーバーのポート (デフォルト: 5000)
- `MLFLOW_EXTERNAL_HOST`: 外部からアクセス可能なMLflowホスト (デフォルト: localhost)

### URL設定
- `VITE_API_BASE_URL`: フロントエンドが使用するAPIベースURL
- `MLFLOW_EXTERNAL_URI`: 外部アクセス用の完全なMLflow URI (http://${MLFLOW_EXTERNAL_HOST}:${MLFLOW_EXTERNAL_PORT})
- `VITE_MLFLOW_DIRECT_URL`: フロントエンドからMLflowへの直接アクセスURL (${MLFLOW_EXTERNAL_URI}から設定)
- `MLFLOW_HOST_URI`: バックエンドがMLflowに接続するための内部URI (http://${MLFLOW_HOST}:${MLFLOW_PORT})

### CORS設定
- `CORS_ORIGINS`: APIバックエンドのCORS許可オリジン (デフォルト: *)

## 6. 障害対策と耐性

プラットフォームには以下の障害対策機能が実装されています：

1. **複数接続先試行**:  
   バックエンドのプロキシ機能は複数の接続先URLを順番に試行し、障害時のフェイルオーバーを実現

2. **タイムアウト設定**:  
   各リクエストには適切なタイムアウト値が設定され、長時間の応答待ちを防止

3. **エラーハンドリング**:  
   詳細なエラー情報の提供と適切なステータスコードの返却

4. **標準API診断**:
   MLflowの標準APIエンドポイント（`/api/2.0/mlflow/experiments/list`）を使用した接続状態確認機能

## 7. セキュリティ考慮事項

現在のネットワーク構成におけるセキュリティポイント：

1. **内部ネットワーク**:  
   コンポーネント間の通信は `llm-eval-network` 内に限定され、直接外部からアクセスできない

2. **CORS設定**:  
   クロスオリジンリクエストの制御（現在はすべてのオリジンを許可）

3. **ポートマッピング**:  
   必要なサービスのみを外部に公開し、内部サービスは直接アクセスできない

### セキュリティ向上のための推奨事項

- 本番環境では `CORS_ORIGINS` を特定のドメインに制限
- 必要な場合のみ外部からアクセス可能なポートを公開
- APIキーや認証機構の追加検討
- プロキシサーバー経由でのアクセス制限

## 8. トラブルシューティング

一般的なネットワーク問題と解決方法：

### 接続エラー

1. **フロントエンド-バックエンド間接続エラー**:
   - 環境変数 `VITE_API_BASE_URL` が正しく設定されているか確認
   - バックエンドサービスが起動しているか確認
   - ネットワークポートが正しくマッピングされているか確認

2. **バックエンド-MLflow間接続エラー**:
   - 環境変数 `MLFLOW_HOST_URI`、または `MLFLOW_HOST` と `MLFLOW_PORT` が正しく設定されているか確認
   - MLflowサービスが起動しているか確認
   - MLflowの標準APIエンドポイント `/api/2.0/mlflow/experiments/list` に直接アクセスして接続状態を診断

3. **MLflow直接アクセスエラー**:
   - 環境変数 `MLFLOW_EXTERNAL_URI` が正しく設定されているか確認
   - `MLFLOW_EXTERNAL_HOST` と `MLFLOW_EXTERNAL_PORT` の組み合わせが有効か確認
   - MLflowポート公開設定がdocker-compose.ymlで正しく行われているか確認

### プロキシ問題

1. **MLflowプロキシエラー**:
   - ブラウザコンソールでCORSエラーがないか確認
   - ネットワーク構成がコンテナ間通信を許可しているか確認
   - MLflowのログで詳細エラーを確認

2. **Ollama API接続エラー**:
   - Ollamaサービスが起動しているか確認
   - バックエンドログで接続エラーを確認
   - バックエンド環境変数 `OLLAMA_BASE_URL` が正しく設定されているか確認
   - API応答時間が長い場合はタイムアウト設定を調整
   - 注意: 旧 `/proxy-ollama/` エンドポイントは削除されています

## 9. まとめ

LLM評価プラットフォームのネットワーク構造は、フロントエンド、バックエンド、MLflowの各コンポーネントが連携し、ユーザーに統合されたエクスペリエンスを提供します。環境変数を活用した柔軟な設定と、フォールバックメカニズムによる障害耐性を備えています。

特に注目すべき点として、環境変数の統一と簡素化により、設定の一貫性が向上し、管理が容易になっています。ブラウザからMLflowに直接アクセスする場合は `MLFLOW_EXTERNAL_URI` が使用され、バックエンドからMLflowに接続する場合は優先順位に従って最適な接続方法が選択されます。

このドキュメントで説明したネットワーク構造を理解することで、プラットフォームの効率的な運用とカスタマイズが可能になります。